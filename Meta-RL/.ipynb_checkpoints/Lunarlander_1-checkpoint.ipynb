{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import gym\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "from Agent import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"LunarLander-v2\"\n",
    "TRAIN_ITERATIONS = 10000\n",
    "MAX_EPISODE_LENGTH = 1000\n",
    "TRAJECTORY_BUFFER_SIZE = 32\n",
    "BATCH_SIZE = 16\n",
    "RENDER_EVERY = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Vinay Patel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "state_input (InputLayer)     [(None, 8)]               0         \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "actor_output_layer (Dense)   (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,377\n",
      "Trainable params: 1,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "state_input (InputLayer)        [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims (TensorF [(1, None, 8)]       0           state_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 24)           3168        tf_op_layer_ExpandDims[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 32)           800         lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 32)           1056        dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "advantage_input (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "old_prediction_input (InputLaye [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "actor_output_layer (Dense)      (None, 4)            132         dense2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 5,156\n",
      "Trainable params: 5,156\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "state_input (InputLayer)        [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_1 (Tenso [(1, None, 8)]       0           state_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 24)           3168        tf_op_layer_ExpandDims_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 32)           800         lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 32)           1056        dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "advantage_input (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "old_prediction_input (InputLaye [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "actor_output_layer (Dense)      (None, 4)            132         dense2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 5,156\n",
      "Trainable params: 5,156\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "agent = Agent(env.action_space.n,env.observation_space.shape,BATCH_SIZE)\n",
    "samples_filled = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Vinay Patel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Episodes: 0 Episodic_Reweard: -84.83271636561857 Max_Reward_Achieved: -84.83271636561857\n",
      "Episodes: 1 Episodic_Reweard: -234.2368885740849 Max_Reward_Achieved: -84.83271636561857\n",
      "Episodes: 2 Episodic_Reweard: -238.17514892881533 Max_Reward_Achieved: -84.83271636561857\n",
      "Episodes: 3 Episodic_Reweard: -192.20122032403626 Max_Reward_Achieved: -84.83271636561857\n",
      "Episodes: 4 Episodic_Reweard: -282.7043708305982 Max_Reward_Achieved: -84.83271636561857\n",
      "Episodes: 5 Episodic_Reweard: -210.78750043430745 Max_Reward_Achieved: -84.83271636561857\n",
      "Episodes: 6 Episodic_Reweard: -287.95548513234905 Max_Reward_Achieved: -84.83271636561857\n",
      "Episodes: 7 Episodic_Reweard: -120.04658407439408 Max_Reward_Achieved: -84.83271636561857\n",
      "Episodes: 8 Episodic_Reweard: -103.08244194256372 Max_Reward_Achieved: -84.83271636561857\n",
      "Episodes: 9 Episodic_Reweard: -193.21749376523996 Max_Reward_Achieved: -84.83271636561857\n",
      "Episodes: 10 Episodic_Reweard: -141.98110432145018 Max_Reward_Achieved: -84.83271636561857\n",
      "Episodes: 11 Episodic_Reweard: -409.380340069609 Max_Reward_Achieved: -84.83271636561857\n",
      "Episodes: 12 Episodic_Reweard: -42.88288745166804 Max_Reward_Achieved: -42.88288745166804\n",
      "Episodes: 13 Episodic_Reweard: -155.303342937563 Max_Reward_Achieved: -42.88288745166804\n",
      "Episodes: 14 Episodic_Reweard: -204.68404892234432 Max_Reward_Achieved: -42.88288745166804\n",
      "Episodes: 15 Episodic_Reweard: 20.877249262219323 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 16 Episodic_Reweard: -125.30051067889966 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 17 Episodic_Reweard: -97.42198664756556 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 18 Episodic_Reweard: -123.04501341526232 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 19 Episodic_Reweard: -131.53413667852627 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 20 Episodic_Reweard: -85.6650388675848 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 21 Episodic_Reweard: -213.27411361425965 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 22 Episodic_Reweard: -76.94534986504364 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 23 Episodic_Reweard: -344.05459034897433 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 24 Episodic_Reweard: -86.71300641608387 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 25 Episodic_Reweard: -319.39502370066384 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 26 Episodic_Reweard: -109.9661266762235 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 27 Episodic_Reweard: -62.9107454148495 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 28 Episodic_Reweard: -130.00464212900582 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 29 Episodic_Reweard: -473.56048267865543 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 30 Episodic_Reweard: -244.23265896876356 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 31 Episodic_Reweard: -305.66400816344776 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 32 Episodic_Reweard: -397.96631130186404 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 33 Episodic_Reweard: -87.61971637184642 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 34 Episodic_Reweard: -191.31512170498394 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 35 Episodic_Reweard: -368.4351320176281 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 36 Episodic_Reweard: -374.56593160442304 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 37 Episodic_Reweard: -465.24862667756736 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 38 Episodic_Reweard: -120.4877384271224 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 39 Episodic_Reweard: -221.77199037866617 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 40 Episodic_Reweard: -182.46019951023982 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 41 Episodic_Reweard: -54.6343191577771 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 42 Episodic_Reweard: -79.87579680913147 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 43 Episodic_Reweard: -190.3921292343929 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 44 Episodic_Reweard: -140.54120477975562 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 45 Episodic_Reweard: -143.01599273020122 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 46 Episodic_Reweard: -198.00731628820887 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 47 Episodic_Reweard: -369.4884293339501 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 48 Episodic_Reweard: -281.1950521015814 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 49 Episodic_Reweard: -425.24167283622296 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 50 Episodic_Reweard: -114.13977536083138 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 51 Episodic_Reweard: -324.26177378355635 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 52 Episodic_Reweard: -134.05604412235868 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 53 Episodic_Reweard: -152.5265372700557 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 54 Episodic_Reweard: -472.76068980533637 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 55 Episodic_Reweard: -295.71671789409106 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 56 Episodic_Reweard: -393.1872993736416 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 57 Episodic_Reweard: -165.82677471370485 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 58 Episodic_Reweard: -128.53954302071918 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 59 Episodic_Reweard: -164.98350290768175 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 60 Episodic_Reweard: -415.4299705332826 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 61 Episodic_Reweard: -184.90109706490904 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 62 Episodic_Reweard: -115.43731928439273 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 63 Episodic_Reweard: -118.95766098938091 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 64 Episodic_Reweard: -133.07678406140175 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 65 Episodic_Reweard: -127.17673382580969 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 66 Episodic_Reweard: -245.60761406593488 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 67 Episodic_Reweard: -167.5843466385858 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 68 Episodic_Reweard: -102.39550579738226 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 69 Episodic_Reweard: -182.2349628603787 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 70 Episodic_Reweard: -304.1142952145421 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 71 Episodic_Reweard: -347.872744032269 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 72 Episodic_Reweard: -92.47347564279748 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 73 Episodic_Reweard: -107.05500603196377 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 74 Episodic_Reweard: -97.00686813742692 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 75 Episodic_Reweard: -200.2174227659205 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 76 Episodic_Reweard: -85.32014277886952 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 77 Episodic_Reweard: -175.3978388488426 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 78 Episodic_Reweard: -116.6977698347595 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 79 Episodic_Reweard: -124.93214574115763 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 80 Episodic_Reweard: -271.6317037999901 Max_Reward_Achieved: 20.877249262219323\n",
      "Episodes: 81 Episodic_Reweard: 23.233536699884837 Max_Reward_Achieved: 23.233536699884837\n",
      "Episodes: 82 Episodic_Reweard: -113.24676837937542 Max_Reward_Achieved: 23.233536699884837\n",
      "Episodes: 83 Episodic_Reweard: -239.7703092405902 Max_Reward_Achieved: 23.233536699884837\n",
      "Episodes: 84 Episodic_Reweard: -81.03278221369234 Max_Reward_Achieved: 23.233536699884837\n",
      "Episodes: 85 Episodic_Reweard: -13.276374799202102 Max_Reward_Achieved: 23.233536699884837\n",
      "Episodes: 86 Episodic_Reweard: -109.8011514697474 Max_Reward_Achieved: 23.233536699884837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes: 87 Episodic_Reweard: -249.33386906958117 Max_Reward_Achieved: 23.233536699884837\n",
      "Episodes: 88 Episodic_Reweard: -313.9544474549592 Max_Reward_Achieved: 23.233536699884837\n",
      "Episodes: 89 Episodic_Reweard: -85.27654213614885 Max_Reward_Achieved: 23.233536699884837\n",
      "Episodes: 90 Episodic_Reweard: -194.16692114943072 Max_Reward_Achieved: 23.233536699884837\n",
      "Episodes: 91 Episodic_Reweard: -266.8602743766203 Max_Reward_Achieved: 23.233536699884837\n",
      "Episodes: 92 Episodic_Reweard: -218.83953740681153 Max_Reward_Achieved: 23.233536699884837\n",
      "Episodes: 93 Episodic_Reweard: -175.27281880094702 Max_Reward_Achieved: 23.233536699884837\n",
      "Episodes: 94 Episodic_Reweard: -587.8859012549634 Max_Reward_Achieved: 23.233536699884837\n",
      "Episodes: 95 Episodic_Reweard: -131.7356489024899 Max_Reward_Achieved: 23.233536699884837\n",
      "Episodes: 96 Episodic_Reweard: -138.95667829380739 Max_Reward_Achieved: 23.233536699884837\n",
      "Episodes: 97 Episodic_Reweard: -410.37587799284165 Max_Reward_Achieved: 23.233536699884837\n",
      "Episodes: 98 Episodic_Reweard: -264.7258325806607 Max_Reward_Achieved: 23.233536699884837\n",
      "Episodes: 99 Episodic_Reweard: -71.18873742599317 Max_Reward_Achieved: 23.233536699884837\n",
      "Episodes: 100 Episodic_Reweard: -136.19941894579824 Max_Reward_Achieved: 23.233536699884837\n",
      "Episodes: 101 Episodic_Reweard: -160.55607953541625 Max_Reward_Achieved: 23.233536699884837\n"
     ]
    }
   ],
   "source": [
    "scores_window = deque(maxlen=100)\n",
    "scores = []\n",
    "max_reward = -1000\n",
    "for cnt_episode in range(TRAIN_ITERATIONS):\n",
    "    s = env.reset()\n",
    "    r_sum = 0\n",
    "    for cnt_step in range(MAX_EPISODE_LENGTH):\n",
    "        if cnt_episode % RENDER_EVERY == 0 :\n",
    "            env.render()\n",
    "        a = agent.choose_action(s)\n",
    "        s_, r, done, _ = env.step(a)\n",
    "        r_sum += r\n",
    "        agent.store_transition(s, a, s_, r, done)\n",
    "        samples_filled += 1\n",
    "        if samples_filled % TRAJECTORY_BUFFER_SIZE == 0 and samples_filled != 0:\n",
    "            for _ in range(TRAJECTORY_BUFFER_SIZE // BATCH_SIZE):\n",
    "                agent.train_network()\n",
    "            agent.memory.clear()\n",
    "            samples_filled = 0\n",
    "        s = s_\n",
    "        if done:\n",
    "            break\n",
    "    scores_window.append(r_sum)\n",
    "    scores.append(r_sum)\n",
    "    if np.mean(scores_window)>=250:\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(cnt_episode-100, np.mean(scores_window)))\n",
    "        agent.actor_network.save_weights(str(r_sum)+\"lunar_actor.h5\")\n",
    "        break\n",
    "    max_reward = max(max_reward, r_sum)\n",
    "    print('Episodes:', cnt_episode, 'Episodic_Reweard:', r_sum, 'Max_Reward_Achieved:', max_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
