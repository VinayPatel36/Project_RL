{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import gym\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "from Agent import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"MountainCar-v0\"\n",
    "TRAIN_ITERATIONS = 5000\n",
    "MAX_EPISODE_LENGTH = 1000\n",
    "TRAJECTORY_BUFFER_SIZE = 32\n",
    "BATCH_SIZE = 16\n",
    "RENDER_EVERY = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Vinay Patel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "state_input (InputLayer)     [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "actor_output_layer (Dense)   (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,185\n",
      "Trainable params: 1,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "state_input (InputLayer)        [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims (TensorF [(1, None, 2)]       0           state_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 24)           2592        tf_op_layer_ExpandDims[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 32)           800         lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 32)           1056        dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "advantage_input (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "old_prediction_input (InputLaye [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "actor_output_layer (Dense)      (None, 3)            99          dense2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 4,547\n",
      "Trainable params: 4,547\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "state_input (InputLayer)        [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_1 (Tenso [(1, None, 2)]       0           state_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 24)           2592        tf_op_layer_ExpandDims_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 32)           800         lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 32)           1056        dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "advantage_input (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "old_prediction_input (InputLaye [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "actor_output_layer (Dense)      (None, 3)            99          dense2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 4,547\n",
      "Trainable params: 4,547\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "agent = Agent(env.action_space.n,env.observation_space.shape,BATCH_SIZE)\n",
    "samples_filled = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Vinay Patel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Episodes: 0 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 1 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 2 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 3 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 4 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 5 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 6 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 7 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 8 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 9 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 10 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 11 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 12 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 13 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 14 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 15 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 16 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 17 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 18 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 19 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 20 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 21 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 22 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 23 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 24 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 25 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 26 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 27 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 28 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 29 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 30 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 31 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 32 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 33 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 34 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 35 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 36 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 37 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 38 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 39 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 40 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 41 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 42 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 43 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 44 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n",
      "Episodes: 45 Episodic_Reweard: -200.0 Max_Reward_Achieved: -200.0\n"
     ]
    }
   ],
   "source": [
    "scores_window = deque(maxlen=100)\n",
    "scores = []\n",
    "max_reward = -500\n",
    "for cnt_episode in range(TRAIN_ITERATIONS):\n",
    "    s = env.reset()\n",
    "    r_sum = 0\n",
    "    for cnt_step in range(MAX_EPISODE_LENGTH):\n",
    "        if cnt_episode % RENDER_EVERY == 0 :\n",
    "            env.render()\n",
    "        a = agent.choose_action(s)\n",
    "        s_, r, done, _ = env.step(a)\n",
    "        r_sum += r\n",
    "        agent.store_transition(s, a, s_, r, done)\n",
    "        samples_filled += 1\n",
    "        if samples_filled % TRAJECTORY_BUFFER_SIZE == 0 and samples_filled != 0:\n",
    "            for _ in range(TRAJECTORY_BUFFER_SIZE // BATCH_SIZE):\n",
    "                agent.train_network()\n",
    "            agent.memory.clear()\n",
    "            samples_filled = 0\n",
    "        s = s_\n",
    "        if done:\n",
    "            break\n",
    "    scores_window.append(r_sum)\n",
    "    scores.append(r_sum)\n",
    "    if np.mean(scores_window)>=-125:\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(cnt_episode-100, np.mean(scores_window)))\n",
    "        agent.actor_model.save_weights(str(r_sum)+\"acrobot_actor.h5\")\n",
    "        break\n",
    "    max_reward = max(max_reward, r_sum)\n",
    "    print('Episodes:', cnt_episode, 'Episodic_Reweard:', r_sum, 'Max_Reward_Achieved:', max_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
